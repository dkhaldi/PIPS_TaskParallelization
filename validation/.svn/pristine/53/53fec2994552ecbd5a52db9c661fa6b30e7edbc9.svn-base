from __future__ import with_statement # this is to work with python2.5
from pyps import *
import re

program = "jacobi01"

# Just in case it existed before:
workspace.delete(program)
with workspace(program + ".c",	"include/p4a_stubs.c", name = program, deleteOnClose=True) as w:
	w.activate(module.transformers_inter_full)
	w.activate(module.interprocedural_summary_precondition)
	w.activate(module.preconditions_inter_full)
	w.activate(module.must_regions)
	w.props.SEMANTICS_COMPUTE_TRANSFORMERS_IN_CONTEXT = True
	w.props.SEMANTICS_FIX_POINT_OPERATOR = "derivative"
	w.props.UNSPAGHETTIFY_TEST_RESTRUCTURING = True
	w.props.UNSPAGHETTIFY_RECURSIVE_DECOMPOSITION = True
	w.props.FOR_TO_DO_LOOP_IN_CONTROLIZER = True
	w.props.KERNEL_LOAD_STORE_VAR_PREFIX = ""
	       # Some temporary hack to have this parallelized loop:
## 	       /* Erase the memory, in case the image is not big enough: */
## #pragma omp parallel for private(j)
##    for(i = 0; i <= 500; i += 1)
## #pragma omp parallel for
##       for(j = 0; j <= 500; j += 1)
##          space[i][j] = 0;                                               /*0045*/
	w.props.ALIASING_ACROSS_IO_STREAMS = False

	w.props.CONSTANT_PATH_EFFECTS = False
	w.props.PRETTYPRINT_STATEMENT_NUMBER = True

	# Skip module name of P4A runtime:
	skip_p4a_runtime_and_compilation_unit_re = re.compile("P4A_.*|.*!")
	def is_not_p4a_runtime(module):
		#print module.name
		return not skip_p4a_runtime_and_compilation_unit_re.match(module.name)

	mn = w.filter(is_not_p4a_runtime)

	#for i in mn:
	#	print i.name

	mn.privatize_module(concurrent=True)

	mn.display()
	mn.display(activate="PRINT_CODE_REGIONS")


	# mn.localize_declaration()

	# mn.display(activate="PRINT_CODE_PRECONDITIONS")

	mn.coarse_grain_parallelization(concurrent=True)
	mn.display()

	w["get_data"].display()


	# First, only generate the launchers to work on them later. They are
        # generated by outlining all the parallel loops:
        mn.gpu_ify(GPU_USE_WRAPPER = False,
		   GPU_USE_KERNEL = False,
		   concurrent=True)
	mn.display()

	#setproperty KERNEL_LOAD_STORE_ALLOCATE_FUNCTION "P4A_ACCEL_MALLOC"
	#setproperty KERNEL_LOAD_STORE_DEALLOCATE_FUNCTION "P4A_ACCEL_FREE"
	#setproperty KERNEL_LOAD_STORE_LOAD_FUNCTION "P4A_COPY_TO_ACCEL"
	#setproperty KERNEL_LOAD_STORE_STORE_FUNCTION "P4A_COPY_FROM_ACCEL"

	# Isolate kernels by using the fact that all the generated kernels have
	# their name beginning with "p4a_":
	kernel_launcher_filter_re = re.compile("p4a_kernel_launcher_.*[^!]$")
	kernel_launchers = w.filter(lambda m: kernel_launcher_filter_re.match(m.name))

        # Normalize all loops in kernels to suit hardware iteration spaces:
        kernel_launchers.loop_normalize(
            # Loop normalize for the C language and GPU friendly
            LOOP_NORMALIZE_ONE_INCREMENT = True,
            # Arrays start at 0 in C, so the iteration loops:
            LOOP_NORMALIZE_LOWER_BOUND = 0,
            # It is legal in the following by construction (...Hmmm to verify)
            LOOP_NORMALIZE_SKIP_INDEX_SIDE_EFFECT = True,
            concurrent=True)

	# Since the privatization of a module does not change
        # privatization of other modules, use concurrent=True (capply) to
        # apply them without requiring pipsmake to carefully rebuild
        # dependent resources:
        kernel_launchers.privatize_module(concurrent=True)
        # Idem for this phase:
        kernel_launchers.coarse_grain_parallelization(concurrent=True)

        # Add iteration space decorations and insert iteration clamping
        # into the launchers onto the outer parallel loop nests:
        kernel_launchers.gpu_loop_nest_annotate(concurrent=True)

        # End to generate the wrappers and kernel contents, but not the
        # launchers that have already been generated:
        kernel_launchers.gpu_ify(GPU_USE_LAUNCHER = False,
                                 concurrent=True)

        #kernels.display()
	# Add communication around all the call site of the kernels:
	kernel_launchers.kernel_load_store(concurrent=True,
                                           ISOLATE_STATEMENT_EVEN_NON_LOCAL = True)
	map(lambda m: m.callers.display(),kernel_launchers)

	# Inline back the kernel into the wrapper, since CUDA can only deal with
	# local functions if they are in the same file as the caller (by inlining
	# them, by the way... :-) )
	kernel_filter_re = re.compile("p4a_kernel_\\d+$")
	kernels = w.filter(lambda m: kernel_filter_re.match(m.name))
	kernels.inlining()

	# Display the wrappers to see the work done:
	kernel_wrapper_filter_re = re.compile("p4a_kernel_wrapper_\\d+$")
	kernel_wrappers = w.filter(lambda m: kernel_wrapper_filter_re.match(m.name))
	kernel_wrappers.display()

	# Instead, do a global loop normalization above:
	#kernels.loop_normalize()
	#kernels.use_def_elimination()
	#display PRINTED_FILE[p4a_kernel_launcher_0,p4a_kernel_launcher_1,p4a_kernel_launcher_2,p4a_kernel_launcher_3,p4a_kernel_launcher_4]

	#w.all.suppress_dead_code()
	#w["main"].display()

	w["main"].prepend_comment(PREPEND_COMMENT = "// Prepend here P4A_init_accel")

	# Unsplit resulting code
	w.all.unsplit()

	# Generating P4A code:
	#os.system("cd " + program + ".database; p4a_post_processor.py Src/*.c")
